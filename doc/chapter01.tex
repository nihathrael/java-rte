
\section{Implementations for parts 1, 2 and 3}

In the first three parts we implement several systems that determine if 
a hypothesis is entailed by a text.


\subsection{Basic framework}

We created a framework that enables us to make, use and evaluate
a variety of RTE-systems in an easy way. So we have all systems of all parts
of the project embedded in one big test environment.
It loads the syntactically parsed data and wraps it in an object oriented structure 
\textit{Text}. It is able to provide the data either as a linked graph or as a 
list of words.

To judge the performance of our systems we decided to implement our own evaluation
code. The huge amount of file operations, that goes along with using the provided 
python script has a big impact on the execution time.

To achieve a common basis for evaluation and easy implementation of new systems we make use of 
object oriented techniques. All of our RTE-systems are derived from the interface
\textit{IEntailmentRecognizer}, that requires functionality to estimate the entailment
propability of two given \textit{Text} instances.

Further we implemented an abstract class \textit{BasicMatcher}, that is the basis for a
category of RTE-systems, that determine the entailment by comparing the sentences word by word.
Thus we are able to create new systems of this category by implementing only one procedure, 
that chooses or calculates the desired property.

\subsection{Systems in part 1}

With the help of our \textit{BasicMatcher} class it was fairly easy to create most systems
of part 1. For each of\textit{Word, Lemma and Part of Speech matching} we could use the 
\textit{BasicMatcher} and extract and compare the particular part, by implementing the
abstract function.

Considering that this kind of analysis ignores most of the semantics the results are quite
impressive. These quite simple implementations still give better results than some 
pretty complicated techniques. Combined with the check for equal polarity, 
\textit{Lemma matching} gives the third best results of all.

\paragraph{BLEU algorithm}
The \textit{BLEU} algorithm, uses n-grams largely, so we implemented the functionality to
calculate them in a seperate class to be able to use it for different purposes.
The \textit{BLEU} algorithm, determines the number of shared n-grams and weights them
according to the length. The descrption suggests to use 4 as maximum for n and to weigh
the grams of different length by using the arithmetic mean. In our implementation we leave
the maximum n as parameter and add an alternative weighing technique. By this we can use
try different setups to figure out which works best. 

Surprisingly the test which used a maximum n of 2, which degenerates the algorithm to a
bigram matcher worked best with a result of 61.875\%.

\subsection{Systems in part 2}

- cost functions
- test case

\subsection{Systems in part 3}
For part 3 we implemented a learning algorithm provided by the Mahout\footnote{Mahout website:
\url{http://mahout.apache.org/}} library. Mahout is a powerful and scalable machine learning library, providing
algorithms for recommendations, clustering, and classification.

We implemented the \textit{BasicMahoutMatcher} by using
the \textit{OnlineLogisticRegression} algorithm. The results were not very promising at first and it is fairly easy to
get really bad results from the machine learner if the parameters are not correctly set or the wrong features are
selected. The following features are used for the \textit{BasicMahoutMatcher}:

\begin{itemize}
    \item Lemma Matching
    \item IDF Lemma Matching
    \item Lemma+POS Matching
    \item BleuScore
    \item 2-Gram Overlap
\end{itemize}

The resulting score is not very good, as no semantic information and almost no structural information is used. The
\textit{BasicMahoutMatcher} scores ~56.4\%.

\subsubsection{Evaluating the Machine Learner}
We implemented a 10-fold cross-validation to verify if our changes to the features improved the matcher or made the
results worse. The code for this can be found in the \textit{Main} class under the function name
\textit{crossValidate(ArrayList<THPair> pairs, IMachineLearnerRecognizer mlearner)}. We also used the 10-fold cross
validation to find the best threshold for the machine-learner results. The Mahout classifier returns probabilites for
each category, therefore we needed to find out the best threshold here as well. We used the average of all 10 runs.


